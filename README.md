# APIIngest - OpenAPI to LLM-Ready Markdown Converter

ğŸš€ **Full-stack web application** that converts OpenAPI YAML/JSON specifications into structured, LLM-friendly markdown format optimized for AI coding assistants.

[![Deploy to Koyeb](https://img.shields.io/badge/Deploy%20to-Koyeb-blue)](https://www.koyeb.com/)

## âœ¨ Features

### Converter Features
**Dereferences `$ref` schemas** â€” Inline references for readability  
**Surfaces authentication** â€” Security schemes pulled into each endpoint  
**Base URLs highlighted** â€” Server information prominently displayed  
**Runnable examples** â€” Auto-generated curl commands with placeholders  
**Strict separators** â€” Delimiters prevent boundary confusion  (Inspired by GitIngest, see below)
**Tag grouping** â€” Organized by tags, alphabetically sorted  
**Type normalization** â€” Clean type display (string (uuid), array<User>, etc.)  
**Token-aware** â€” Designed to keep endpoint chunks under 2-4K tokens  

## ğŸš€ Quick Start

### Local Development

```bash
# Clone the repository
git clone https://github.com/mohidbt/apic.git
cd apic

# Terminal 1 - Start Backend
cd backend
pip install -r requirements.txt
python main.py

# Terminal 2 - Start Frontend
cd frontend
npm install
npm run dev
```

Then open http://localhost:3000 in your browser!

### Using the Converter

1. **Web Interface** (Recommended)
   - Open http://localhost:3000
   - Drag and drop your OpenAPI YAML/JSON file
   - Click "Convert to Markdown"
   - Download starts automatically!

2. **Command Line**
   ```bash
   cd backend
   python transformation.py ../examples/APIs.guru-swagger.yaml
   ```

3. **API Endpoint**
   ```bash
   curl -X POST http://localhost:8000/convert \
     -F "file=@openapi-spec.yaml" \
     -o output.md
   ```

## ğŸ“ Project Structure

```
apiingest/
â”œâ”€â”€ backend/               # FastAPI server
â”‚   â”œâ”€â”€ main.py           # API server with file upload
â”‚   â”œâ”€â”€ transformation.py # Core OpenAPIâ†’Markdown converter
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ README.md         # Backend documentation
â”œâ”€â”€ frontend/             # Next.js web application
â”‚   â”œâ”€â”€ src/              # React components and pages
â”‚   â”œâ”€â”€ public/           # Static assets
â”‚   â”œâ”€â”€ package.json      # Node dependencies
â”‚   â””â”€â”€ .env.example      # Environment variables template
â”œâ”€â”€ examples/             # Example OpenAPI specifications
â”‚   â”œâ”€â”€ APIs.guru-swagger.yaml
â”‚   â”œâ”€â”€ APIs.guru-swagger.json
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ SETUP.md         # Detailed setup guide
â”‚   â”œâ”€â”€ DEPLOYMENT.md    # Koyeb deployment instructions
â”‚   â””â”€â”€ QUICK_REFERENCE.md
â”œâ”€â”€ .koyeb/              # Deployment configuration
â”‚   â””â”€â”€ config.yaml
â””â”€â”€ README.md            # This file
```

## ğŸ“– Output Format

The generated markdown follows a strict, LLM-optimized structure:

### Header Section
```markdown
# API Title
**Version:** 1.0.0

Brief description...

## Base URLs
  - https://api.example.com â€” Production
  - https://sandbox.api.example.com â€” Sandbox

## Authentication
  - bearerAuth: HTTP BEARER
  - apiKey: API Key (header: X-API-Key)
```

### Endpoint Blocks

Each endpoint uses strict delimiters (inspired by Gitingest):

```
================================================================================
ENDPOINT: [GET] /users/{id}
TAGS: Users
SUMMARY: Get a user by ID
DESCRIPTION: Retrieves detailed user information...
AUTH: Bearer token

REQUEST
  Path params:
  - id (string (uuid), required)
  Query params:
  - verbose (boolean, optional)
  Body:
  none

RESPONSES
  - 200 (application/json): Success
    - id: string (uuid, required)
    - name: string (required)
    - email: string (email, required)
  - 404: User not found

EXAMPLE (curl)
curl -X GET \
  "https://api.example.com/users/123?verbose=true" \
  -H "Authorization: Bearer $TOKEN"
================================================================================
```

## ğŸ¯ Why This Format?

### LLM Benefits

1. **Strict Delimiters** â€” `=` bars prevent AI from confusing endpoint boundaries
2. **Dereferenced Schemas** â€” No need to chase `$ref` pointers during code generation
3. **Inline Auth** â€” Security requirements visible per-endpoint
4. **Runnable Examples** â€” Copy-paste curl commands with placeholder variables
5. **Normalized Types** â€” Consistent type display helps pattern recognition
6. **Tag Grouping** â€” Logical organization mimics developer thinking
7. **Token-Optimized** â€” Chunks designed to fit in context windows

### Inspired by Gitingest

This format borrows from [Gitingest](https://gitingest.com/)'s approach:
- Three-section structure (header, TOC, content)
- Repeated delimiters between entries
- Stable, deterministic ordering
- Noise filtering for clarity

## ğŸŒ Deployment

### Deploy to Koyeb

**Two deployment options:**

#### Option 1: Single Service (Recommended for Simple Deployments)
Deploy backend + frontend together in one container.

See **[KOYEB_SINGLE_SERVICE.md](KOYEB_SINGLE_SERVICE.md)** for detailed instructions.

**Quick steps:**
1. Use root-level `Dockerfile`
2. Set environment variables (PORT, ALLOWED_ORIGINS, etc.)
3. Deploy as one service
4. Both services run together via supervisord

**Environment Variables:**
```bash
PORT=8000
ALLOWED_ORIGINS=https://{{ KOYEB_PUBLIC_DOMAIN }}/
NEXT_PUBLIC_API_URL=http://localhost:8000
NODE_ENV=production
```

#### Option 2: Two Separate Services (Recommended for Production)
Deploy backend and frontend as independent services.

See **[docs/DEPLOYMENT.md](docs/DEPLOYMENT.md)** for complete instructions.

**Quick Overview:**
1. Deploy backend service (FastAPI) from `backend/` directory
2. Deploy frontend service (Next.js) from `frontend/` directory
3. Set environment variables for both services
4. Update CORS settings with deployed URLs

## ğŸ› ï¸ Technology Stack

### Backend
- **FastAPI** â€” Modern Python web framework
- **uvicorn** â€” ASGI server
- **PyYAML** â€” YAML parsing
- **Python 3.8+**

### Frontend
- **Next.js 15** â€” React framework
- **TypeScript** â€” Type safety
- **Tailwind CSS** â€” Styling
- **shadcn/ui** â€” UI components
- **Sonner** â€” Toast notifications

## ğŸ“š Documentation

- **[SETUP.md](docs/SETUP.md)** â€” Detailed installation and configuration
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md)** â€” Koyeb deployment guide
- **[QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md)** â€” Command reference
- **[Backend README](backend/README.md)** â€” API documentation
- **[Examples](examples/README.md)** â€” Example specifications

## ğŸ§ª Testing

Try it with the included examples:

```bash
# Backend directory
cd backend

# Convert example spec
python transformation.py ../examples/APIs.guru-swagger.yaml

# Start API server for testing
python main.py
```

## ğŸ¤ Contributing

Contributions welcome! This project prioritizes LLM readability over human aesthetics.

**Key principles:**
1. **Strict structure** â€” Consistent delimiters and ordering
2. **Inline critical info** â€” Auth, types, examples per endpoint
3. **Token efficiency** â€” Truncate where necessary
4. **Deterministic output** â€” Same input = same output

## ğŸ“ License

MIT â€” Feel free to use, modify, and distribute.

## ğŸ”— Related Projects

- [Gitingest](https://gitingest.com/) â€” LLM-optimized code repository digests

## ğŸ’¡ Use Cases

- **AI Coding Assistants** â€” Feed API docs to Claude, GPT, etc.
- **API Documentation** â€” Generate readable docs from OpenAPI specs
- **Developer Onboarding** â€” Clear, structured API references
- **RAG Systems** â€” Token-optimized chunks for retrieval
- **Code Generation** â€” Help AI understand your API structure

## ğŸ› Issues & Support

Found a bug or have a question?
- Open an issue on GitHub
- Check existing [documentation](docs/)
- Try with [example files](examples/)

## ğŸ‰ Acknowledgments

Built with inspiration from:
- Gitingest's LLM-friendly formatting approach
- OpenAPI Specification community
- Modern web development best practices

---

**Made with â¤ï¸ for developers working with LLMs**
